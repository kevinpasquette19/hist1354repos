{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO3_B5vsx144"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMpD95Drx5hz"
      },
      "source": [
        "In this lesson, we will explore how to use Python for text analysis by working with a trial transcript from the *Proceedings of the Old Bailey*. Text analysis is an essential tool for historians, allowing us to uncover patterns and insights from historical documents that would be difficult to identify manually.\n",
        "\n",
        "We will begin by scraping the trial transcript from the *Old Bailey Online*, a valuable digital archive containing the records of over 197,000 criminal trials held at the Central Criminal Court in London between 1674 and 1913. These records provide a rich source of information about crime, justice, and society in early modern London."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47AZV0v7yDpK"
      },
      "source": [
        "This hands-on lesson will teach you how to:\n",
        "\n",
        "1.   Scrape text data from a webpage.\n",
        "2.   Clean the text to make it suitable for analysis.\n",
        "3.   Generate and visualize word frequencies to reveal trends in the language used during the trial.\n",
        "\n",
        "By the end of the lesson, you will be able to apply these basic text analysis techniques to other historical documents, helping you to engage with historical sources in new and exciting ways.\n",
        "\n",
        "This lesson was inspired by [this sequence of lessons](https://programminghistorian.org/en/lessons/from-html-to-list-of-words-1) on the [*Programming Historian*](https://programminghistorian.org/)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG2YdJk7yOV6"
      },
      "source": [
        "## About the *Proceedings of the Old Bailey*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6ySCRNgyTQc"
      },
      "source": [
        "The *Proceedings of the Old Bailey* is a major online resource for historians, providing access to detailed accounts of trials held at London's central criminal court from 1674 to 1913. It includes witness testimony, defense statements, verdicts, and sentencing information. The website also offers powerful search tools, allowing users to investigate topics ranging from crime and punishment to class, gender, and race in early modern London.\n",
        "\n",
        "The transcript we will analyze today is from the trial of Benjamin Bowsey, a participant in the 1780 [Gordon Riots](https://en.wikipedia.org/wiki/Gordon_Riots), accused of rioting and the destruction of property. This document, like many others in the *Old Bailey Online*, offers valuable insights into the social and political tensions of the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7G37ij9yzeD"
      },
      "source": [
        "# Step 1: Scraping text from the trial transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "asyabf4fxW9-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.6)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "324. BENJAMIN BOWSEY (a blackmoor ) was indicted for that he together with five hundred other persons and more, did, unlawfully, riotously, and tumultuously assemble on the 6th of June to the disturbance of the public peace and did begin to demolish and pull down the dwelling house of Richard Akerman , against the form of the statute, &c.   ROSE JENNINGS , Esq. sworn. Had you any occasion to be in this part of the town, on the 6th of June in the evening? - I dined with my brother who lives oppos\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "%pip install bs4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Open the HTML file and parse it\n",
        "# REMEMBER TO CHANGE THIS PATH SO THAT IT POINTS AT A FILE YOU HAVE ACCESS TO!\n",
        "with open('/Users/kevinpasquette/Documents/GitHub/hist1354repos/riot_trial.html', 'r', encoding='utf-8') as file:\n",
        "    soup = BeautifulSoup(file, 'html.parser')\n",
        "\n",
        "# Extract all the paragraph tags which contain the main text of the trial\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Concatenate the text content of all paragraphs into one string\n",
        "raw_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "# Print the first 500 characters to see what the text looks like\n",
        "print(raw_text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBeUJhy4zQee"
      },
      "source": [
        "\n",
        "*   `BeautifulSoup` is used to parse the HTML content.\n",
        "\n",
        "*   We locate all the `<p>` tags since visual inspection of the HTML indicated that the trial transcript is contained within those tags.\n",
        "\n",
        "*   The content of each paragraph is extracted, combined into a single string (`raw_text`), and we print the first 500 characters to verify the extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRFZVSGaz4I0"
      },
      "source": [
        "# Step 2: Cleaning the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-WHDMDRz6ZR"
      },
      "source": [
        "The next step is to clean the scraped text. We will remove unwanted characters, such as special symbols, punctuation, and extra spaces, and make all text lowercase for uniformity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3XhQ-rW_z8nJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement nltk.corpus (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for nltk.corpus\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: stopwords in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.0.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/kevinpasquette/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/kevinpasquette/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Clean the extracted raw text with stop word removal\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Print the first 500 characters of the cleaned text to check\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_text[:\u001b[38;5;241m500\u001b[39m])\n",
            "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     23\u001b[0m words \u001b[38;5;241m=\u001b[39m cleaned\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Remove stop words using NLTK's stop word list for English\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     27\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Return the cleaned and filtered text as a single string\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/kevinpasquette/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%pip install re\n",
        "%pip install nltk\n",
        "%pip install nltk.corpus\n",
        "%pip install stopwords\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords from NLTK if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a more advanced cleaning function with stop word removal\n",
        "def clean_text(text):\n",
        "    # Remove any non-alphabetic characters (numbers, punctuation, etc.)\n",
        "    cleaned = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    cleaned = cleaned.lower()\n",
        "    # Remove extra whitespace\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "\n",
        "    # Split the text into individual words\n",
        "    words = cleaned.split()\n",
        "\n",
        "    # Remove stop words using NLTK's stop word list for English\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Return the cleaned and filtered text as a single string\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Clean the extracted raw text with stop word removal\n",
        "cleaned_text = clean_text(raw_text)\n",
        "\n",
        "# Print the first 500 characters of the cleaned text to check\n",
        "print(cleaned_text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJv1RmHG0DK2"
      },
      "source": [
        "\n",
        "*   The re.sub() function is used to remove any character that is not a letter or whitespace.\n",
        "\n",
        "*   The clean_text function also converts the text to lowercase and removes excess spaces.\n",
        "\n",
        "*   After cleaning the text by removing non-alphabetic characters and converting it to lowercase, we split the text into individual words.\n",
        "\n",
        "*   We use the `nltk.corpus.stopwords` list to remove common English stop words.\n",
        "\n",
        "*   The cleaned text is returned as a single string of meaningful words, ready for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5wYpeWA0W1B"
      },
      "source": [
        "# Step 3: Word frequency analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdaiHzCV0wDy"
      },
      "source": [
        "Now that we have cleaned the text, we can generate a list of word frequencies. This will help us understand which words are most common in the trial transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hehCnoEL0yM7"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Split the cleaned text into individual words\n",
        "words = cleaned_text.split()\n",
        "\n",
        "# Use Counter to count the occurrences of each word\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# Display the 10 most common words\n",
        "word_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfnsQAQp1eE4"
      },
      "source": [
        "*   We split the cleaned text into words using `.split()`, which creates a list of words.\n",
        "\n",
        "*   The `Counter` class from the `collections` module counts the frequency of each word.\n",
        "\n",
        "*   We print the 10 most common words using `most_common(10)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr_VaHD01tRv"
      },
      "source": [
        "# Visualizing word frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa2Nof1K1wkV"
      },
      "source": [
        "If you want to visualize the word frequencies, you can use the `matplotlib` library to create a bar chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNLYBgMg1z4b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the 10 most common words and their frequencies\n",
        "common_words = word_freq.most_common(10)\n",
        "words, counts = zip(*common_words)\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(words, counts)\n",
        "plt.title('Top 10 Most Common Words in Trial Transcript')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeAdFAXU13ip"
      },
      "source": [
        "*   This code takes the top 10 most frequent words and their counts.\n",
        "\n",
        "*   It uses `matplotlib` to generate a simple bar chart visualizing the word frequencies.\n",
        "\n",
        "*   The `xticks(rotation=45)` rotates the word labels to make them easier to read."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzPaaMDD2QME"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-9iVSmd2Ra-"
      },
      "source": [
        "In this lesson, you learned how to scrape, clean, and analyze a historical trial transcript using Python. The techniques covered here provide a foundation for text analysis in digital history, enabling you to extract meaningful insights from primary source documents. As you continue working with historical texts, consider how these computational methods can be applied to different types of documents and how they might complement more traditional historical approaches.\n",
        "\n",
        "The combination of digital tools and historical research opens new possibilities for exploring the past, allowing us to engage with sources in ways that were previously unimaginable."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
